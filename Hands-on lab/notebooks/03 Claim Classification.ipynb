{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Claims classification with Keras: The Python Deep Learning Library\n\nIn this notebook, you will train a classification model for claim text that will predict `1` if the claim is an auto insurance claim or `0` if it is a home insurance claim. The model will be built using a type of DNN called the Long Short-Term Memory (LSTM) recurrent neural network using TensorFlow via the Keras library.\n\nThis notebook will walk you through the text analytic process that consists of:\n\n- Example word analogy with Glove word embeddings\n- Vectorizing training data using GloVe word embeddings\n- Creating and training a LSTM based classifier model\n- Using the model to predict classifications"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Prepare modules\n\nThis notebook will use the Keras library to build and train the classifier."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import string\nimport re\nimport os\nimport numpy as np\nimport pandas as pd\nimport urllib.request\n\nimport tensorflow as tf\nimport keras\nfrom keras import models, layers, optimizers, regularizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Embedding, LSTM\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\n\nprint('Keras version: ', keras.__version__)\nprint('Tensorflow version: ', tf.__version__)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Let's download the pretrained GloVe word embeddings and load them in this notebook.**\n\nThis will create a `dictionary` of size **400,000** words, and the corresponding `GloVe word vectors` for words in the dictionary. Each word vector is of size: 50, thus the dimensionality of the word embeddings used here is **50**.\n\n*The next cell might take couple of minutes to run*"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "words_list_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n                  'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/wordsList.npy')\n\nword_vectors_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n                    'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/wordVectors.npy')\n\nword_vectors_dir = './word_vectors'\n\nos.makedirs(word_vectors_dir, exist_ok=True)\nurllib.request.urlretrieve(words_list_url, os.path.join(word_vectors_dir, 'wordsList.npy'))\nurllib.request.urlretrieve(word_vectors_url, os.path.join(word_vectors_dir, 'wordVectors.npy'))\n\ndictionary = np.load(os.path.join(word_vectors_dir, 'wordsList.npy'))\ndictionary = dictionary.tolist()\ndictionary = [word.decode('UTF-8') for word in dictionary]\nprint('Loaded the dictionary! Dictionary size: ', len(dictionary))\n\nword_vectors = np.load(os.path.join(word_vectors_dir, 'wordVectors.npy'))\nprint ('Loaded the word vectors! Shape of the word vectors: ', word_vectors.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Create the word contractions map. The map is going to used to expand contractions in our corpus (for example \"can't\" becomes \"cannot\").**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "contractions_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n                    'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/contractions.xlsx')\ncontractions_df = pd.read_excel(contractions_url)\ncontractions = dict(zip(contractions_df.original, contractions_df.expanded))\nprint('Review first 10 entries from the contractions map')\nprint(contractions_df.head(10))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Word analogy example with GloVe word embeddings"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "GloVe represents each word in the dictionary as a vector. We can use word vectors for predicting word analogies. \n\nSee example below that solves the following analogy: **father->mother :: king->?**"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Cosine similarity is a measure used to evaluate how similar two words are. This helper function takes vectors of two words and returns their cosine similarity that range from -1 to 1. For synonyms the cosine similarity will be close to 1 and for antonyms the cosine similarity will be close to -1."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def cosine_similarity(u, v):\n    dot = u.dot(v)\n    norm_u = np.linalg.norm(u)\n    norm_v = np.linalg.norm(v)\n    cosine_similarity = dot/norm_u/norm_v\n    return cosine_similarity",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let’s review the vector for the words **father**, **mother**, and **king**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "father = word_vectors[dictionary.index('father')]\nmother = word_vectors[dictionary.index('mother')]\nking = word_vectors[dictionary.index('king')]\nprint(father)\nprint('')\nprint(mother)\nprint('')\nprint(king)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To solve for the analogy, we need to solve for x in the following equation:\n\n**mother – father = x - king**\n\nThus, **x = mother - father + king**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x = mother - father + king",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Next, we will find the word whose word vector is closest to the vector x computed above**\n\nTo limit the computation cost, we will identify the best word from a list of possible answers instead of searching the entire dictionary."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "answers = ['women', 'prince', 'princess', 'england', 'goddess', 'diva', 'empress', \n           'female', 'lady', 'monarch', 'title', 'queen', 'sovereign', 'ruler', \n           'male', 'crown', 'majesty', 'royal', 'cleopatra', 'elizabeth', 'victoria', \n           'throne', 'internet', 'sky', 'machine', 'learning', 'fairy']\n\ndf = pd.DataFrame(columns = ['word', 'cosine_similarity'])\n\n# Find the similarity of each word in answers with x\nfor w in answers:\n    sim = cosine_similarity(word_vectors[dictionary.index(w)], x)   \n    df = df.append({'word': w, 'cosine_similarity': sim}, ignore_index=True)\n    \ndf.sort_values(['cosine_similarity'], ascending=False, inplace=True)\n\nprint(df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**From the results above, you can observe the vector for the word `queen` is most similar to the vector `x`.**"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Prepare the training data\n\nContoso Ltd has provided a small document containing examples of the text they receive as claim text. They have provided this in a text file with one line per sample claim.\n\nRun the following cell to download and examine the contents of the file. Take a moment to read the claims (you may find some of them rather comical!)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data_location = './data'\nbase_data_url = 'https://databricksdemostore.blob.core.windows.net/data/05.03/'\nfilesToDownload = ['claims_text.txt', 'claims_labels.txt']\n\nos.makedirs(data_location, exist_ok=True)\n\nfor file in filesToDownload:\n    data_url = os.path.join(base_data_url, file)\n    local_file_path = os.path.join(data_location, file)\n    urllib.request.urlretrieve(data_url, local_file_path)\n    print('Downloaded file: ', file)\n    \nclaims_corpus = [claim for claim in open(os.path.join(data_location, 'claims_text.txt'))]\nclaims_corpus",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In addition to the claims sample, Contoso Ltd has also provided a document that labels each of the sample claims provided as either 0 (\"home insurance claim\") or 1 (\"auto insurance claim\"). This to is presented as a text file with one row per sample, presented in the same order as the claim text.\n\nRun the following cell to examine the contents of the supplied claims_labels.txt file:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "labels = [int(re.sub(\"\\n\", \"\", label)) for label in open(os.path.join(data_location, 'claims_labels.txt'))]\nprint(len(labels))\nprint(labels[0:5]) # first 5 labels\nprint(labels[-5:]) # last 5 labels",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As you can see from the above output, the values are integers 0 or 1. In order to use these as labels with which to train our model, we need to convert these integer values to categorical values (think of them like enum's from other programming languages).\n\nWe can use the to_categorical method from `keras.utils` to convert these value into binary categorical values. Run the following cell:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "labels = to_categorical(labels, 2)\nprint(labels.shape)\nprint()\nprint(labels[0:2]) # first 2 categorical labels\nprint()\nprint(labels[-2:]) # last 2 categorical labels",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now that we have our claims text and labels loaded, we are ready to begin our first step in the text analytics process, which is to normalize the text."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Process the claims corpus\n\n- Lowercase all words\n- Expand contractions (for example \"can't\" becomes \"cannot\")\n- Remove special characters (like punctuation)\n- Convert the list of words in the claims text to a list of corresponding indices of those words in the dictionary. Note that the order of the words as they appear in the written claims is maintained.\n\nRun the next cell to process the claims corpus."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def remove_special_characters(token):\n    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n    filtered_token = pattern.sub('', token)\n    return filtered_token\n\ndef convert_to_indices(corpus, dictionary, c_map, unk_word_index = 399999):\n    sequences = []\n    for i in range(len(corpus)):\n        tokens = corpus[i].split()\n        sequence = []\n        for word in tokens:\n            word = word.lower()\n            if word in c_map:\n                resolved_words = c_map[word].split()\n                for resolved_word in resolved_words:\n                    try:\n                        word_index = dictionary.index(resolved_word)\n                        sequence.append(word_index)\n                    except ValueError:\n                        sequence.append(unk_word_index) #Vector for unkown words\n            else:\n                try:\n                    clean_word = remove_special_characters(word)\n                    if len(clean_word) > 0:\n                        word_index = dictionary.index(clean_word)\n                        sequence.append(word_index)\n                except ValueError:\n                    sequence.append(unk_word_index) #Vector for unkown words\n        sequences.append(sequence)\n    return sequences\n\nclaims_corpus_indices = convert_to_indices(claims_corpus, dictionary, contractions)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Review the indices of one sample claim**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(remove_special_characters(claims_corpus[5]).split())\nprint()\nprint('Ordered list of indices for the above claim')\nprint(claims_corpus_indices[5])\nprint('')\nprint('For example, the index of second word in the claims text \\\"pedestrian\\\" is: ', dictionary.index('pedestrian'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Create fixed length vectors**\n\nThe number of words used in a claim, vary with the claim. We need to create the input vectors of fixed size. We will use the utility function `pad_sequences` from `keras.preprocessing.sequence` to help us create fixed size vector (size = 125) of word indices."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "maxSeqLength = 125\n\nX = pad_sequences(claims_corpus_indices, maxlen=maxSeqLength, padding='pre', truncating='post')\n\nprint('Review the new fixed size vector for a sample claim')\nprint(remove_special_characters(claims_corpus[5]).split())\nprint()\nprint(X[5])\nprint('')\nprint('Lenght of the vector: ', len(X[5]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Build the LSTM recurrent neural network\n\nNow that you have preprocessed the input features from training text data, you are ready to build the classifier. In this case, we will build a LSTM recurrent neural network. The network will have a word embedding layer that will convert the word indices to GloVe word vectors. The GloVe word vectors are then passed to the LSTM layer, followed by a binary classifier output layer.\n\nRun the following cell to build the structure for your neural network:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "embedding_layer = Embedding(word_vectors.shape[0],\n                            word_vectors.shape[1],\n                            weights=[word_vectors],\n                            input_length=maxSeqLength,\n                            trainable=False)\nmodel = Sequential()\nmodel.add(embedding_layer)\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Train the neural network"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "First, we will split the data into two sets: (1) training set and (2) validation or test set. The validation set accuracy will be used to measure the performance of the model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We will use the `Adam` optimization algorithm to train the model. Also, given that the problem is of type `Binary Classification`, we are using the `Sigmoid` activation function for the output layer and the `Binary Crossentropy` as the loss function."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "opt = keras.optimizers.Adam(lr=0.001)\nmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now we are ready to let the DNN learn by fitting it against our training data and labels. We have defined the batch size and the number of epochs for our training.\n\nRun the following cell to fit your model against the data:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "epochs = 100\nbatch_size = 16\nmodel.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Take a look at the final output for the value \"val_accuracy\". This stands for validation set accuracy. If you think of random chance as having a 50% accuracy, is you model better than random?\n\nIt's OK if it's not much better then random at this point- this is only your first model! The typical data science process would continue with many more iterations taking different actions to improve the model accuracy, including:\n- Acquiring more labeled documents for training\n- Regularization to prevent overfitting\n- Adjusting the model hyperparameters, such as the number of layers, number of nodes per layer, and learning rate"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Test classifying claims\n\nNow that you have constructed a model, try it out against a set of claims. Recall that we need to first preprocess the text.\n\nRun the following cell to prepare our test data:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_claim = ['I crashed my car into a pole.', \n              'The flood ruined my house.', \n              'I lost control of my car and fell in the river.']\n\ntest_claim_indices = convert_to_indices(test_claim, dictionary, contractions)\ntest_data = pad_sequences(test_claim_indices, maxlen=maxSeqLength, padding='pre', truncating='post')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now use the model to predict the classification:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pred = model.predict(test_data)\npred_label = pred.argmax(axis=1)\npred_df = pd.DataFrame(np.column_stack((pred,pred_label)), columns=['class_0', 'class_1', 'label'])\npred_df.label = pred_df.label.astype(int)\nprint('Predictions')\npred_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Model exporting and importing\n\nNow that you have a working model, you need export the trained model to a file so that it can be used downstream by the deployed web service.\n\n*The next two cells might take couple of minutes to run*\n\nTo export the model run the following cell:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import joblib\n\noutput_folder = './output'\nmodel_filename = 'final_model.hdf5'\nos.makedirs(output_folder, exist_ok=True)\nmodel.save(os.path.join(output_folder, model_filename))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To test re-loading the model into the same Notebook instance, run the following cell:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.models import load_model\nloaded_model = load_model(os.path.join(output_folder, model_filename))\nloaded_model.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As before you can use the model to run predictions.\n\nRun the following cells to try the prediction with the re-loaded model:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pred = loaded_model.predict(test_data)\npred_label = pred.argmax(axis=1)\npred_df = pd.DataFrame(np.column_stack((pred,pred_label)), columns=['class_0', 'class_1', 'label'])\npred_df.label = pred_df.label.astype(int)\nprint('Predictions')\npred_df",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}