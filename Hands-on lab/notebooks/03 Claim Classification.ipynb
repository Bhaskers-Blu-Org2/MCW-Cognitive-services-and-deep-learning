{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Claims classification with Keras: The Python Deep Learning Library\n\nIn this notebook, you will train a classification model for claim text that will predict `1` if the claim is an auto insurance claim or `0` if it is a home insurance claim. The model will be built using a Deep Neural Network using TensorFlow via the Keras library.\n\nThis notebook will walk you through a simplified text analytic process that consists of:\n\nNormalizing the training text data\nExtracting the features of the training text as vectors\nCreating and training a DNN based classifier model\nUsing the model to predict classifications"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Prepare modules\n\nThis notebook will use the Keras library to build and train the classifier. In addition, it relies on a supplied helper library that performs common text analytic functions, called textanalytics."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import re\nimport nltk\nimport uuid\n\nimport os\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nimport keras\nfrom keras import models, layers, optimizers, regularizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.utils import to_categorical\n\nprint('Keras version: ', keras.__version__)\nprint('Tensorflow version: ', tf.__version__)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Let's copy locally all the data needed by this notebook.**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import urllib.request\n\ndata_location = './data'\nbase_data_url = 'https://databricksdemostore.blob.core.windows.net/data/05.03/'\nfilesToDownload = ['claims_labels.txt', 'claims_text.txt', 'contractions.py', 'textanalytics.py']\n\nos.makedirs(data_location, exist_ok=True)\n\nfor file in filesToDownload:\n    data_url = os.path.join(base_data_url, file)\n    local_file_path = os.path.join(data_location, file)\n    urllib.request.urlretrieve(data_url, local_file_path)\n    print('Downloaded file: ', file)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's import some helper code that resides in python .py files.\nNote: This is how you can bring your extra Python code residing on an arbitrary location into the notebook."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import nltk\nimport sys\nnltk.download('stopwords')\nnltk.download('punkt')\nsys.path.append(data_location)\nimport textanalytics as ta",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Prepare the training data\n\nContoso Ltd has provided a small document containing examples of the text they receive as claim text. They have provided this in a text file with one line per sample claim.\n\nRun the following cell to examine the contents of the file. Take a moment to read the claims (you may find some of them rather comical!)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "claims_corpus = [claim for claim in open(os.path.join(data_location, 'claims_text.txt'))]\nclaims_corpus",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In addition to the claims sample, Contoso Ltd has also provided a document that labels each of the sample claims provided as either 0 (\"home insurance claim\") or 1 (\"auto insurance claim\"). This to is presented as a text file with one row per sample, presented in the same order as the claim text.\n\nRun the following cell to examine the contents of the supplied claims_labels.txt file:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "labels = [int(re.sub(\"\\n\", \"\", label)) for label in open(os.path.join(data_location, 'claims_labels.txt'))]\nprint(len(labels))\nprint(labels[0:5]) # first 5 labels\nprint(labels[-5:]) # last 5 labels",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As you can see from the above output, the values are integers 0 or 1. In order to use these as labels with which to train our model, we need to convert these integer values to categorical values (think of them like enum's from other programming languages).\n\nWe can use the to_categorical method from `keras.utils` to convert these value into binary categorical values. Run the following cell:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "labels = to_categorical(labels, 2)\nprint(labels.shape)\nprint()\nprint(labels[0:2]) # first 2 categorical labels\nprint()\nprint(labels[-2:]) # last 2 categorical labels",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now that we have our claims text and labels loaded, we are ready to begin our first step in the text analytics process, which is to normalize the text."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Normalize the claims corpus\n\nThe textanalytics module supplied takes care of implementing our desired normalization logic. In summary, what it does is:\n\n- Expand contractions (for example \"can't\" becomes \"cannot\")\n- Lowercase all text\n- Remove special characters (like punctuation)\n- Remove stop words (these are words like \"a\", \"an\", \"the\" that add no value)\n\nRun the following command and observe how the claim text is modified:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "norm_corpus = ta.normalize_corpus(claims_corpus)\nnorm_corpus",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "## Feature extraction: vectorize the claims corpus\n\nFeature extraction in text analytics has the goal of creating a numeric representation of the textual documents.\n\nDuring feature extraction, a “vocabulary” of unique words is identified and each word becomes a column in the output. In other words, the table is as wide as the vocabulary.\n\nEach row represents a document. The value in each cell is typically a measure of the relative importance of that word in the document, where if a word from the vocabular does not appear that cell has a zero value in that column. In other words, the table is as tall as all of the documents in the corpus.\n\nThis approach enables machine learning algorithms, which operate against arrays of numbers, to also operate against text becasue each text document is now represented as an array of numbers.\n\nDeep learning algorithms operate on tensors, which are also vectors (or arrays of numbers) and so this approach is also valid for preparing text for use with a deep learning algorithm.\n\nRun the following command to see what the vectorized version of the claims in norm_corpus looks like:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "vectorizer, tfidf_matrix = ta.build_feature_matrix(norm_corpus) \ndata = tfidf_matrix.toarray()\nprint(data.shape)\ndata",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Observe in the above output, that the shape (the dimensions) of the data is 40 rows by 258 columns. You should interpret this as our vectorizer determined that there 258 words in the vocabulary learned from all documents in the set. There are 40 documents in our training set, hence the vectorized output has 40 rows (one array of numbers for each document)."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Build the neural network\n\nNow that you have normalized and extracted the features from training text data, you are ready to build the classifier. In this case, we will build a simple neural network. The network will be 2 layers deep and each node in one layer is connected to every other node in a subsequent layer. This is what is meant by fully connected.\n\nRun the following cell to build the structure for your neural network:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "np.random.seed(125)\nmodel = Sequential()\nmodel.add(Dense(60, input_dim=data.shape[1], kernel_regularizer=regularizers.l2(0.02)))\nmodel.add(Activation('relu'))\nmodel.add(Dense(2))\nmodel.add(Activation('sigmoid'))\n\nmodel.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Train the neural network"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "First, we will split the data into two sets: (1) training set and (2) validation or test set. The validation set accuracy will be used to measure the performance of the model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We will use the `Adam` optimization algorithm to train the model. Also, given that the problem is of type `Binary Classification`, we are using the `Sigmoid` activation function for the output layer and the `Binary Crossentropy` as the loss function."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "opt = keras.optimizers.Adam(lr=0.001)\nmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now we are ready to let the DNN learn by fitting it against our training data and labels. We have defined the batch size and the number of epochs for our training.\n\nRun the following cell to fit your model against the data:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "epochs = 100\nbatch_size = 16\nmodel.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Take a look at the final output for the value \"val_acc\". This stands for validation set accuracy. If you think of random chance as having a 50% accuracy, is you model better than random?\n\nIt's OK if it's not much better then random at this point- this is only your first model! The typical data science process would continue with many more iterations taking different actions to improve the model accuracy, including:\n- Acquiring more labeled documents for training\n- Preparing the text with more sophisticated techniques such as lemmatization\n- Regularization to prevent overfitting\n- Adjusting the model hyperparameters, such as the number of layers, number of nodes per layer, and learning rate"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Test classifying claims\n\nNow that you have constructed a model, try it out against a set of claims. Recall that we need to normalize and featurize the text using the exact same pipeline we used during training.\n\nRun the following cell to prepare our test data:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_claim = ['I crashed my car into a pole.', \n              'The flood ruined my house.', \n              'I lost control of my car and fell in the river.']\ntest_claim = ta.normalize_corpus(test_claim)\ntest_claim = vectorizer.transform(test_claim)\n\ntest_claim = test_claim.toarray()\nprint(test_claim.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now use the model to predict the classification:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pred = model.predict(test_claim)\npred_label = pred.argmax(axis=1)\npred_df = pd.DataFrame(np.column_stack((pred,pred_label)), columns=['class_0', 'class_1', 'label'])\npred_df.label = pred_df.label.astype(int)\nprint('Predictions')\npred_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Model exporting and importing\n\nNow that you have a working model, you need export the trained model, and the vectorizer to a file so that they can be used downstream by the deployed web service.\n\nTo export the model and the vectorizer, run the following cell:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.externals import joblib\n\noutput_folder = './output'\nmodel_filename = 'final_model.hdf5'\nos.makedirs(output_folder, exist_ok=True)\nmodel.save(os.path.join(output_folder, model_filename))\n\nvectorizer_name = 'vectorizer'\njoblib.dump(vectorizer, os.path.join(output_folder, vectorizer_name))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To test re-loading the model into the same Notebook instance, run the following cell:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.models import load_model\nloaded_model = load_model(os.path.join(output_folder, model_filename))\nloaded_model.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As before you can use the model to run predictions.\n\nRun the following cells to try the prediction with the re-loaded model:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pred = loaded_model.predict(test_claim)\npred_label = pred.argmax(axis=1)\npred_df = pd.DataFrame(np.column_stack((pred,pred_label)), columns=['class_0', 'class_1', 'label'])\npred_df.label = pred_df.label.astype(int)\nprint('Predictions')\npred_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}