{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Converting a Keras model to ONNX\n\nIn the steps that follow, you will convert Keras model you just trained to the ONNX format. This will enable you to use this model for classification in a very broad range of environments, outside of Azure Databricks including:\n\n- Web services\n- iOS and Android mobile apps\n- Windows apps\n- IoT devices\n\nFurthermore, ONNX runtimes and libraries are also designed to maximize performance on some of the best hardware in the industry. In this lab, we will compare the Inference performance of the ONNX vs Keras models.\n\nFirst we will load the trained Keras model from file, and then convert the model to ONNX."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Load the Keras Model\n\nLoad the saved Keras model. We will convert the Keras model to ONNX format."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(125)\n\nfrom keras.models import load_model\nimport joblib\n\noutput_folder = './output'\nmodel_filename = 'final_model.hdf5'\n\nkeras_model = load_model(os.path.join(output_folder, model_filename))\nprint(keras_model.summary())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Convert to ONNX\n\nConvert the loaded Keras model to ONNX format, and save the ONNX model to the deployment folder."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import onnxmltools\n\ndeployment_folder = 'deploy'\nonnx_export_folder = 'onnx'\n\n# Convert the Keras model to ONNX\nonnx_model_name = 'claim_classifier.onnx'\nconverted_model = onnxmltools.convert_keras(keras_model, onnx_model_name, target_opset=7)\n\n# Save the model locally...\nonnx_model_path = os.path.join(deployment_folder, onnx_export_folder)\nos.makedirs(onnx_model_path, exist_ok=True)\nonnxmltools.utils.save_model(converted_model, os.path.join(onnx_model_path,onnx_model_name))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Make Inference using the ONNX Model\n\n- Create an ONNX runtime InferenceSession\n- Review the expected input shape to make inferences\n- Prepare test data\n- Make inferences using both the ONNX and the Keras Model on the test data"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### ONNX Runtime InferenceSession"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import onnxruntime\n# Load the ONNX model and observe the expected input shape\nonnx_session = onnxruntime.InferenceSession(\n    os.path.join(os.path.join(deployment_folder, onnx_export_folder), onnx_model_name))\ninput_name = onnx_session.get_inputs()[0].name\noutput_name = onnx_session.get_outputs()[0].name\nprint('Expected input shape: ', onnx_session.get_inputs()[0].shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Prepare test data"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Load the GloVe word vectors**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "word_vectors_dir = './word_vectors'\n\ndictonary = np.load(os.path.join(word_vectors_dir, 'wordsList.npy'))\ndictonary = dictonary.tolist()\ndictonary = [word.decode('UTF-8') for word in dictonary]\nprint('Loaded the dictonary! Dictonary size: ', len(dictonary))\n\nword_vectors = np.load(os.path.join(word_vectors_dir, 'wordVectors.npy'))\nprint ('Loaded the word vectors! Shape of the word vectors: ', word_vectors.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Create the word contractions map**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "contractions_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n                    'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/contractions.xlsx')\ncontractions_df = pd.read_excel(contractions_url)\ncontractions = dict(zip(contractions_df.original, contractions_df.expanded))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Setup the helper functions to process the test data**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import re\nimport string\n\ndef remove_special_characters(token):\n    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n    filtered_token = pattern.sub('', token)\n    return filtered_token\n\ndef convert_to_indices(corpus, dictonary, c_map, unk_word_index = 399999):\n    sequences = []\n    for i in range(len(corpus)):\n        tokens = corpus[i].split()\n        sequence = []\n        for word in tokens:\n            word = word.lower()\n            if word in c_map:\n                resolved_words = c_map[word].split()\n                for resolved_word in resolved_words:\n                    try:\n                        word_index = dictonary.index(resolved_word)\n                        sequence.append(word_index)\n                    except ValueError:\n                        sequence.append(unk_word_index) #Vector for unkown words\n            else:\n                try:\n                    clean_word = remove_special_characters(word)\n                    if len(clean_word) > 0:\n                        word_index = dictonary.index(clean_word)\n                        sequence.append(word_index)\n                except ValueError:\n                    sequence.append(unk_word_index) #Vector for unkown words\n        sequences.append(sequence)\n    return sequences",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Preprocess the test data**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.preprocessing.sequence import pad_sequences\nmaxSeqLength = 125\n\ntest_claim = ['I crashed my car into a pole.']\n\ntest_claim_indices = convert_to_indices(test_claim, dictonary, contractions)\ntest_data = pad_sequences(test_claim_indices, maxlen=maxSeqLength, padding='pre', truncating='post')\n\n# convert the data type to float\ntest_data_float = np.reshape(test_data.astype(np.float32), (1,maxSeqLength))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Make Inferences\n\nMake inferences using both the ONNX and the Keras Model on the test data"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Run an ONNX session to classify the sample.\nprint('ONNX prediction: ', onnx_session.run([output_name], {input_name : test_data_float}))\n\n# Use Keras to make predictions on the same sample\nprint('Keras prediction: ', keras_model.predict(test_data_float))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Compare Inference Performance: ONNX vs Keras\n\nEvaluate the performance of ONNX and Keras by running the same sample 1,000 times. Run the next three cells and compare the performance in your environment."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Next we will compare the performance of ONNX vs Keras\nimport timeit\nn = 1000",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "start_time = timeit.default_timer()\nfor i in range(n):\n    keras_model.predict(test_data_float)\nkeras_elapsed = timeit.default_timer() - start_time\nprint('Keras performance: ', keras_elapsed)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "start_time = timeit.default_timer()\nfor i in range(n):\n    onnx_session.run([output_name], {input_name : test_data_float})\nonnx_elapsed = timeit.default_timer() - start_time\nprint('ONNX performance: ', onnx_elapsed)\nprint('ONNX is about {} times faster than Keras'.format(round(keras_elapsed/onnx_elapsed)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Deploy ONNX model to Azure Container Instance (ACI)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create and connect to an Azure Machine Learning Workspace\n\nReview the workspace config file saved in the previous notebook."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!cat .azureml/config.json",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Create the `Workspace` from the saved config file**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import azureml.core\n\nprint(azureml.core.VERSION)\n\nfrom azureml.core.workspace import Workspace\n\nws = Workspace.from_config()\nprint(ws)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Register the model with Azure Machine Learning\n\nIn the following, you register the model with Azure Machine Learning (which saves a copy in the cloud)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Register the model and vectorizer\nfrom azureml.core.model import Model\n\nregistered_model_name = 'claim_classifier_onnx'\nonnx_model_path = os.path.join(os.path.join(deployment_folder, onnx_export_folder), onnx_model_name)\n\nregistered_model = Model.register(model_path = onnx_model_path, # this points to a local file\n                       model_name = registered_model_name, # this is the name the model is registered with         \n                       description = \"Claims classification model.\",\n                       workspace = ws)\n\nprint(registered_model.name, registered_model.description, registered_model.version)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create the scoring web service\n\nWhen deploying models for scoring with Azure Machine Learning services, you need to define the code for a simple web service that will load your model and use it for scoring. By convention this service has two methods init which loads the model and run which scores data using the loaded model.\n\nThis scoring service code will later be deployed inside of a specially prepared Docker container."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Save the scoring web service Python file**\n\nNote that the scoring web service needs the registered model: the ONNX model to make inferences."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile scoring_service.py\nimport string\nimport re\nimport os\nimport numpy as np\nimport pandas as pd\nimport urllib.request\nimport json\nimport keras\nfrom keras.preprocessing.sequence import pad_sequences\nimport tensorflow as tf\nfrom azureml.core.model import Model\nimport onnxruntime\n\ndef init():\n\n    global onnx_session\n    global dictonary\n    global contractions\n    \n    try:\n        words_list_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n                          'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/wordsList.npy')\n        word_vectors_dir = './word_vectors'\n        os.makedirs(word_vectors_dir, exist_ok=True)\n        urllib.request.urlretrieve(words_list_url, os.path.join(word_vectors_dir, 'wordsList.npy'))\n        dictonary = np.load(os.path.join(word_vectors_dir, 'wordsList.npy'))\n        dictonary = dictonary.tolist()\n        dictonary = [word.decode('UTF-8') for word in dictonary]\n        print('Loaded the dictonary! Dictonary size: ', len(dictonary))\n        \n        contractions_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n                            'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/contractions.xlsx')\n        contractions_df = pd.read_excel(contractions_url)\n        contractions = dict(zip(contractions_df.original, contractions_df.expanded))\n        print('Loaded contractions!')\n        \n        # Retrieve the path to the model file using the model name\n        onnx_model_name = 'claim_classifier_onnx'\n        onnx_model_path = Model.get_model_path(onnx_model_name)\n        print('onnx_model_path: ', onnx_model_path)\n        \n        onnx_session = onnxruntime.InferenceSession(onnx_model_path)\n        print('Onnx Inference Session Created!')\n        \n    except Exception as e:\n        print(e)\n        \ndef remove_special_characters(token):\n    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n    filtered_token = pattern.sub('', token)\n    return filtered_token\n\ndef convert_to_indices(corpus, dictonary, c_map, unk_word_index = 399999):\n    sequences = []\n    for i in range(len(corpus)):\n        tokens = corpus[i].split()\n        sequence = []\n        for word in tokens:\n            word = word.lower()\n            if word in c_map:\n                resolved_words = c_map[word].split()\n                for resolved_word in resolved_words:\n                    try:\n                        word_index = dictonary.index(resolved_word)\n                        sequence.append(word_index)\n                    except ValueError:\n                        sequence.append(unk_word_index) #Vector for unkown words\n            else:\n                try:\n                    clean_word = remove_special_characters(word)\n                    if len(clean_word) > 0:\n                        word_index = dictonary.index(clean_word)\n                        sequence.append(word_index)\n                except ValueError:\n                    sequence.append(unk_word_index) #Vector for unkown words\n        sequences.append(sequence)\n    return sequences\n\ndef run(raw_data):\n    try:\n        print(\"Received input: \", raw_data)\n        \n        maxSeqLength = 125\n        \n        print('Processing input...')\n        input_data_raw = np.array(json.loads(raw_data))\n        input_data_indices = convert_to_indices(input_data_raw, dictonary, contractions)\n        input_data_padded = pad_sequences(input_data_indices, maxlen=maxSeqLength, padding='pre', truncating='post')\n        # convert the data type to float\n        input_data = np.reshape(input_data_padded.astype(np.float32), (1,maxSeqLength))\n        print('Done processing input.')\n        \n        # Run an ONNX session to classify the input.\n        result = onnx_session.run(None, {onnx_session.get_inputs()[0].name: input_data})[0].argmax(axis=1).item()\n        # return just the classification index (0 or 1)\n        return result\n    except Exception as e:\n        print(e)\n        error = str(e)\n        return error",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Package Model\n\nYour scoring service can have dependencies install by using a Conda environment file. Items listed in this file will be conda or pip installed within the Docker container that is created and thus be available to your scoring web service logic.\n\nBuild a container image that names the scoring service script, the runtime (python or Spark), the conda file, and the registered models.\n\nRun the following cell. This may take between 5-10 minutes to complete."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# create a Conda dependencies environment file\nprint(\"Creating conda dependencies file locally...\")\nfrom azureml.core.conda_dependencies import CondaDependencies\nconda_packages = ['numpy==1.16.4', 'xlrd==1.2.0', 'pandas==0.25.1', 'scikit-learn==0.21.3']\npip_packages = ['azureml-sdk', 'tensorflow==1.13.1', 'keras==2.3.1', 'onnxruntime==1.0.0']\n\nmycondaenv = CondaDependencies.create(conda_packages=conda_packages, pip_packages=pip_packages)\n    \nconda_file = 'dependencies.yml'\nwith open(conda_file, 'w') as f:\n    f.write(mycondaenv.serialize_to_string())\n\nruntime = 'python'\nexecution_script = 'scoring_service.py'\n\n# create container image configuration\nprint(\"Creating container image configuration...\")\nfrom azureml.core.image import ContainerImage\nimage_config = ContainerImage.image_configuration(execution_script = execution_script, \n                                                  runtime = runtime, conda_file = conda_file)\n\n# create the image\nimage_name = 'claim-classifier-image'\n\nfrom azureml.core import Image\nimage = Image.create(name=image_name, models=[registered_model], \n                     image_config=image_config, workspace=ws)\n\n# wait for image creation to finish\nimage.wait_for_creation(show_output=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Deploy Model to ACI\n\nDeploy the webservice from the image created in the previous step.\n\nYou will see output similar to the following when your web service is ready: SucceededACI service creation operation finished, operation \"Succeeded\"\n\nRun the following cell. This takes around 5 minutes to complete."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.webservice import AciWebservice, Webservice\n\naci_config = AciWebservice.deploy_configuration(\n    cpu_cores = 1, \n    memory_gb = 1, \n    tags = {'name': 'Claim Classification'}, \n    description = \"Classifies a claim as home or auto.\")\n\nservice_name = \"claimclassservice\"\n\naci_service = Webservice.deploy_from_image(deployment_config=aci_config, \n                                           image=image, \n                                           name=service_name, \n                                           workspace=ws)\n\naci_service.wait_for_deployment(show_output=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Test Deployment"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Make direct calls on the service object"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import json\n\ntest_claims = ['I crashed my car into a pole.', \n               'The flood ruined my house.', \n               'I lost control of my car and fell in the river.']\n\nfor i in range(len(test_claims)):\n    result = aci_service.run(json.dumps([test_claims[i]]))\n    print('Predicted label for test claim #{} is {}'.format(i+1, result))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Make HTTP calls to test the deployed Web Service\n\nIn order to call the service from a REST client, you need to acquire the scoring URI. Take a note of printed scoring URI, you will need it in the last notebook.\n\nThe default settings used in deploying this service result in a service that does not require authentication, so the scoring URI is the only value you need to call this service."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import requests\n\nurl = aci_service.scoring_uri\nprint('ACI Service: Claim Classification scoring URI is: {}'.format(url))\nheaders = {'Content-Type':'application/json'}\n\nfor i in range(len(test_claims)):\n    response = requests.post(url, json.dumps([test_claims[i]]), headers=headers)\n    print('Predicted label for test claim #{} is {}'.format(i+1, response.text))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}